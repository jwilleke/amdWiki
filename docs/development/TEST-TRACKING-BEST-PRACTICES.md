# Test Tracking Best Practices

**Last Updated:** 2025-12-07

## Overview

This document defines the standardized approach for tracking test failures, successes, and overall test health in the amdWiki project. Following these practices ensures consistent documentation and makes it easier to identify patterns, prioritize fixes, and measure progress.

## Quick Reference

**Where to Track:**

- **Test Status & Strategy:** [KNOWN-TEST-ISSUES.md](./KNOWN-TEST-ISSUES.md)
- **Detailed Test Results:** Jest JSON reports in `test-results/`
- **Session Work Log:** [docs/project_log.md](../project_log.md)
- **Active Work Status:** [AGENTS.md](../../AGENTS.md)

## Test Tracking System

### 1. Central Documentation (KNOWN-TEST-ISSUES.md)

**Purpose:** Single source of truth for test suite health and fix strategy

**Required Sections:**

```markdown
## Test Status Summary

**Current Results:**
- Test Suites: X failed, Y passed, Z total
- Tests: A failed, B skipped, C passed, D total
- Coverage: Available via `npm run test:coverage`

**Progress:**
- âœ… Completed items with test counts
- ðŸ”§ Items in progress
```

**Categories of Failures:**

Each category should document:

- **Pattern:** What the error looks like
- **Example:** Actual error message
- **Files Affected:** List of test files
- **Fix Strategy:** How to address it

**Specific Test Files:**

Organize by priority (High/Medium/Low) with:

- File name and purpose
- Issue description
- Impact level
- Effort estimate
- Current status (ðŸ”§ In Progress, âœ… Fixed, â¸ï¸ Deferred)

**Progress Tracking Table:**

```markdown
| Date | Failing Suites | Passing Suites | Passing Tests | Notes |
|------|---------------|----------------|---------------|-------|
| YYYY-MM-DD | X | Y | Z | Brief description of changes |
```

### 2. Test Results Storage (Automated)

**Location:** `test-results/` directory (gitignored)

**Format:** JSON reports generated by Jest

**Commands:**

```bash
# Generate JSON report
npm test -- --json --outputFile=test-results/$(date +%Y-%m-%d).json

# Generate coverage report
npm run test:coverage
```

**What to Track:**

- Test execution time
- Memory usage
- Flaky test detection
- Coverage metrics

### 3. Session Work Log (project_log.md)

**When:** After completing test-related work in a session

**Required Information:**

```markdown
## YYYY-MM-DD-##

**Agent:** Claude Code (Sonnet 4.5)

**Subject:** Brief description

**Test Status:**

**Before fixes:**
- Specific test file: X/Y passing
- Overall: A failing suites, B passing, C passing tests

**After fixes:**
- Specific test file: X/Y passing
- Overall: A failing suites, B passing, C passing tests
- Pass rate improved: X% â†’ Y%

**Changes Made:**
1. **file/path** - What changed
   - Bullet point details

**Key Insights:**
- Learnings about the codebase, test patterns, etc.
```

### 4. Active Status (AGENTS.md)

**Purpose:** Quick snapshot of current test work

**Update When:**

- Starting new test fixes
- Completing test suite fixes
- Hitting blockers

**Format:**

```markdown
#### Active Work (Session YYYY-MM-DD-##)

**Test Suite Improvements:**

- âœ… TestFile.test.js - COMPLETE (X/X tests passing)
- ðŸ”§ TestFile.test.js - IN PROGRESS (X/Y tests passing)

**Current Test Status:**
- Test Suites: X failed, Y passed, Z total (N% pass rate)
- Tests: A failed, B skipped, C passed, D total (N% pass rate)
```

## Best Practices

### Documentation Standards

#### 1. Use Consistent Terminology

- **Test Suite** = A test file (e.g., `UserManager.test.js`)
- **Test** = Individual test case within a suite
- **Pass Rate** = (passing / total) Ã— 100%
- **Coverage** = Code coverage percentage

#### 2. Update All Four Tracking Locations

When fixing tests, update in this order:

1. Fix the test file
2. Run tests and verify results
3. Update KNOWN-TEST-ISSUES.md (status, progress table)
4. Update AGENTS.md (current status)
5. Add session entry to project_log.md

#### 3. Categorize Test Failures

Use these standard categories:

- **Configuration/Mock Issues** - Missing or incorrect mocks
- **Provider/Dependency Loading** - Dynamic require failures
- **File System/Path Issues** - Missing files, wrong paths
- **Parser/Handler Configuration** - Initialization problems
- **Worker Process Issues** - Memory leaks, crashes
- **API Mismatch** - Tests calling wrong methods
- **Async/Timing Issues** - Race conditions, timeouts

#### 4. Priority Levels

- **High Priority:** Core functionality, security, critical paths
- **Medium Priority:** Features, rendering, common operations
- **Low Priority:** Edge cases, nice-to-have features

### Workflow for Fixing Tests

#### Step 1: Identify the Issue

```bash
# Run specific test file
npm test -- path/to/test.js

# Run with verbose output
npm test -- path/to/test.js --verbose

# Check for specific pattern
npm test -- path/to/test.js 2>&1 | grep -A 5 "Error:"
```

#### Step 2: Document Before State

Update KNOWN-TEST-ISSUES.md with current status:

```markdown
### Test File Name

**Status:** ðŸ”§ In Progress

**Current:** X/Y tests passing

**Issues:**
- List specific failures
```

#### Step 3: Fix and Verify

Make changes, then:

```bash
# Verify specific test
npm test -- path/to/test.js

# Verify no regression
npm test

# Check coverage
npm run test:coverage
```

#### Step 4: Document After State

Update all tracking locations:

- KNOWN-TEST-ISSUES.md: Mark as âœ… Fixed, update progress table
- AGENTS.md: Update current status
- project_log.md: Add detailed session entry

### Quick Wins Strategy

Prioritize fixes by:

1. **Impact** - How critical is the functionality?
2. **Effort** - How long will it take?
3. **Blockers** - Does it unblock other tests?

Mark quick wins (< 10 min effort, high impact):

```markdown
## Quick Wins

- [ ] **test-file.test.js** - Just needs ConfigurationManager mock
- [ ] **other-test.test.js** - Simple import path fix
```

### Anti-Patterns to Avoid

#### âŒ Don't: Skip Updating Documentation

```javascript
// Fix test but don't update KNOWN-TEST-ISSUES.md
// Result: No one knows what's fixed
```

#### âœ… Do: Update All Tracking Locations

```javascript
// Fix test
// Update KNOWN-TEST-ISSUES.md progress table
// Update AGENTS.md status
// Add project_log.md entry
```

#### âŒ Don't: Batch Too Many Fixes

```markdown
Fixed 20 test files
- Result: Can't track which fix did what
```

#### âœ… Do: Fix and Document Incrementally

```markdown
Session 1: Fixed UserManager.test.js (31 tests)
Session 2: Fixed PageManager.test.js (26 tests)
- Result: Clear progress tracking
```

#### âŒ Don't: Use Vague Descriptions

```markdown
Fixed some tests
- Result: No one knows what changed
```

#### âœ… Do: Be Specific

```markdown
Fixed UserManager.test.js (31/31 tests passing)
- Changed from mocking validateCredentials() to getUser() + verifyPassword()
- Added PolicyManager mock with correct subject structure
```

## Test Health Metrics

### Key Performance Indicators (KPIs)

Track these metrics over time:

1. **Test Suite Pass Rate:** (passing suites / total suites) Ã— 100%
2. **Individual Test Pass Rate:** (passing tests / total tests) Ã— 100%
3. **Code Coverage:** Percentage of code covered by tests
4. **Average Fix Time:** Days between identifying and fixing issues
5. **Regression Rate:** Number of previously passing tests that break

### Health Thresholds

- **Healthy:** > 90% pass rate
- **Warning:** 70-90% pass rate
- **Critical:** < 70% pass rate

### Progress Goals

From KNOWN-TEST-ISSUES.md:

> **Goal:** Reduce failing suites to < 10 within 1 month through incremental fixes during normal development.

Track weekly progress:

```markdown
| Week | Failing Suites | Passing Tests | Trend |
|------|---------------|---------------|-------|
| W49  | 46            | 993           | â¬‡ï¸    |
| W50  | 40            | 1169          | â¬†ï¸    |
```

## Integration with Development Workflow

### Daily Development

- Run tests before committing: `npm test`
- Check coverage on feature branches: `npm run test:coverage`
- Document any new test failures immediately

### Weekly Maintenance

- Pick 1-2 failing test files from KNOWN-TEST-ISSUES.md
- Fix them completely
- Update progress tracking
- Review test health metrics

### Monthly Review

- Update Test Status Summary
- Review progress toward goals
- Identify systemic issues
- Update fix strategy if needed

## Tools and Automation

### Jest Configuration

**Current setup:**

- Global test setup: `jest.setup.js`
- Coverage thresholds: Configured in `jest.config.js`
- Test timeout: 120000ms for long-running tests

### Recommended Tools

Based on industry best practices ([TestRail Defect Tracking Guide](https://www.testrail.com/blog/defect-tracking-guide/), [BrowserStack Test Management](https://www.browserstack.com/test-management/what-is-test-management)):

**For Larger Teams:**

- **TestRail** - Test case management
- **Jira** - Issue tracking integration
- **Allure** - Beautiful test reports

**For Solo/Small Teams (amdWiki current approach):**

- **Markdown Documentation** - KNOWN-TEST-ISSUES.md (âœ… Current)
- **Jest JSON Reports** - Automated metrics
- **GitHub Issues** - Track test-related bugs
- **Git History** - Track test changes over time

### Future Enhancements

Consider adding:

1. **Automated Test Reports:** GitHub Actions job that posts test results to PRs
2. **Coverage Badges:** Display coverage percentage in README
3. **Flaky Test Detection:** Track tests that fail intermittently
4. **Test Execution Time Tracking:** Identify slow tests

## Examples from amdWiki

### Example 1: Complete Test Fix Documentation

**KNOWN-TEST-ISSUES.md:**

```markdown
3. ~~**UserManager.test.js**~~ - âœ… FIXED (31 tests passing)
   - Completely rewrote to test proxy behavior
   - Fixed authentication logic to use verifyPassword instead of validateCredentials
   - Added PolicyManager mocking for permission tests
```

**project_log.md:**

```markdown
## 2025-12-07-02

**Test Status:**

**Before fixes:**
- UserManager.test.js: 0/67 passing (completely broken)

**After fixes:**
- UserManager.test.js: 31/31 passing (100%)

**Key Insights:**
1. Authentication Flow: UserManager doesn't delegate to validateCredentials()
2. Permission System: Uses PolicyManager with policy-based access control
```

### Example 2: Quick Win Documentation

**KNOWN-TEST-ISSUES.md:**

```markdown
## Quick Wins

- [x] **SchemaManager.test.js** - âœ… FIXED (rewrote to match actual API)
- [x] **FilterChain.test.js** - âœ… FIXED (adjusted assertions)
```

## References

Industry best practices and resources:

- [Jest Testing Best Practices - GeekyAnts](https://geekyants.com/blog/writing-effective-unit-tests-best-practices)
- [JavaScript Testing Best Practices - Goldbergyoni](https://github.com/goldbergyoni/javascript-testing-best-practices)
- [Defect Tracking Guide - TestRail](https://www.testrail.com/blog/defect-tracking-guide/)
- [Test Management Best Practices - AgileTest](https://agiletest.app/test-management-best-practices/)
- [What is Test Management - BrowserStack](https://www.browserstack.com/test-management/what-is-test-management)

---

**Maintained by:** AI Agents working on amdWiki
**Review Frequency:** Monthly or when test strategy changes
