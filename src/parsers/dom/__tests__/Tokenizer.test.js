/**
 * Unit tests for Tokenizer
 * Tests character-by-character reading, position tracking, and utility methods
 */

const { Tokenizer, TokenType } = require('../Tokenizer');

describe('Tokenizer', () => {
  describe('Constructor', () => {
    test('creates tokenizer with input string', () => {
      const tokenizer = new Tokenizer('test input');
      expect(tokenizer.input).toBe('test input');
      expect(tokenizer.length).toBe(10);
      expect(tokenizer.position).toBe(0);
      expect(tokenizer.line).toBe(1);
      expect(tokenizer.column).toBe(1);
    });

    test('handles empty input', () => {
      const tokenizer = new Tokenizer('');
      expect(tokenizer.input).toBe('');
      expect(tokenizer.length).toBe(0);
      expect(tokenizer.isEOF()).toBe(true);
    });

    test('handles null input', () => {
      const tokenizer = new Tokenizer(null);
      expect(tokenizer.input).toBe('');
      expect(tokenizer.length).toBe(0);
    });
  });

  describe('nextChar()', () => {
    test('reads characters sequentially', () => {
      const tokenizer = new Tokenizer('abc');
      expect(tokenizer.nextChar()).toBe('a');
      expect(tokenizer.nextChar()).toBe('b');
      expect(tokenizer.nextChar()).toBe('c');
      expect(tokenizer.nextChar()).toBe(null);
    });

    test('advances position correctly', () => {
      const tokenizer = new Tokenizer('abc');
      expect(tokenizer.position).toBe(0);
      tokenizer.nextChar();
      expect(tokenizer.position).toBe(1);
      tokenizer.nextChar();
      expect(tokenizer.position).toBe(2);
      tokenizer.nextChar();
      expect(tokenizer.position).toBe(3);
    });

    test('tracks line numbers on newlines', () => {
      const tokenizer = new Tokenizer('line1\nline2\nline3');
      expect(tokenizer.line).toBe(1);

      // Read through first line
      while (tokenizer.peekChar() !== '\n') {
        tokenizer.nextChar();
      }
      tokenizer.nextChar(); // consume newline
      expect(tokenizer.line).toBe(2);

      // Read through second line
      while (tokenizer.peekChar() !== '\n') {
        tokenizer.nextChar();
      }
      tokenizer.nextChar(); // consume newline
      expect(tokenizer.line).toBe(3);
    });

    test('tracks column numbers', () => {
      const tokenizer = new Tokenizer('abc\ndef');
      expect(tokenizer.column).toBe(1);
      tokenizer.nextChar(); // a
      expect(tokenizer.column).toBe(2);
      tokenizer.nextChar(); // b
      expect(tokenizer.column).toBe(3);
      tokenizer.nextChar(); // c
      expect(tokenizer.column).toBe(4);
      tokenizer.nextChar(); // \n
      expect(tokenizer.column).toBe(1);
      expect(tokenizer.line).toBe(2);
      tokenizer.nextChar(); // d
      expect(tokenizer.column).toBe(2);
    });

    test('handles ASCII and basic Unicode', () => {
      const tokenizer = new Tokenizer('hÃ©llo wÃ¶rld');
      expect(tokenizer.nextChar()).toBe('h');
      expect(tokenizer.nextChar()).toBe('Ã©');
      expect(tokenizer.nextChar()).toBe('l');
    });

    // Note: JavaScript strings use UTF-16 encoding where emojis are surrogate pairs
    // This means emojis take 2 string positions. This is acceptable for wiki markup.
    test.skip('emoji handling (known limitation)', () => {
      const tokenizer = new Tokenizer('ðŸ˜€emojiðŸŽ‰');
      // This would fail because 'ðŸ˜€' is a surrogate pair
      expect(tokenizer.nextChar()).toBe('ðŸ˜€');
    });
  });

  describe('peekChar()', () => {
    test('returns current character without advancing', () => {
      const tokenizer = new Tokenizer('abc');
      expect(tokenizer.peekChar()).toBe('a');
      expect(tokenizer.peekChar()).toBe('a');
      expect(tokenizer.position).toBe(0);
    });

    test('returns null at EOF', () => {
      const tokenizer = new Tokenizer('a');
      tokenizer.nextChar();
      expect(tokenizer.peekChar()).toBe(null);
    });

    test('works with pushback buffer', () => {
      const tokenizer = new Tokenizer('abc');
      const char = tokenizer.nextChar();
      tokenizer.pushBack(char);
      expect(tokenizer.peekChar()).toBe('a');
    });
  });

  describe('pushBack()', () => {
    test('allows undoing character read', () => {
      const tokenizer = new Tokenizer('abc');
      const char1 = tokenizer.nextChar(); // 'a'
      const char2 = tokenizer.nextChar(); // 'b'

      tokenizer.pushBack(char2);
      expect(tokenizer.nextChar()).toBe('b');
      expect(tokenizer.nextChar()).toBe('c');
    });

    test('handles multiple pushbacks', () => {
      const tokenizer = new Tokenizer('abc');
      const char1 = tokenizer.nextChar(); // 'a'
      const char2 = tokenizer.nextChar(); // 'b'
      const char3 = tokenizer.nextChar(); // 'c'

      tokenizer.pushBack(char3);
      tokenizer.pushBack(char2);
      tokenizer.pushBack(char1);

      expect(tokenizer.nextChar()).toBe('a');
      expect(tokenizer.nextChar()).toBe('b');
      expect(tokenizer.nextChar()).toBe('c');
    });

    test('restores position correctly', () => {
      const tokenizer = new Tokenizer('abc');
      tokenizer.nextChar(); // position = 1
      tokenizer.nextChar(); // position = 2
      expect(tokenizer.position).toBe(2);

      tokenizer.pushBack('b');
      expect(tokenizer.position).toBe(1);
    });

    test('handles null gracefully', () => {
      const tokenizer = new Tokenizer('a');
      tokenizer.pushBack(null);
      expect(tokenizer.nextChar()).toBe('a');
    });

    test('restores line and column on newline pushback', () => {
      const tokenizer = new Tokenizer('abc\ndef');

      // Read to newline
      tokenizer.nextChar(); // a
      tokenizer.nextChar(); // b
      tokenizer.nextChar(); // c
      const nl = tokenizer.nextChar(); // \n
      expect(tokenizer.line).toBe(2);
      expect(tokenizer.column).toBe(1);

      tokenizer.pushBack(nl);
      expect(tokenizer.line).toBe(1);
    });
  });

  describe('peekAhead()', () => {
    test('returns next N characters without consuming', () => {
      const tokenizer = new Tokenizer('abcdef');
      expect(tokenizer.peekAhead(3)).toBe('abc');
      expect(tokenizer.position).toBe(0);
      expect(tokenizer.peekChar()).toBe('a');
    });

    test('handles EOF in peek ahead', () => {
      const tokenizer = new Tokenizer('ab');
      expect(tokenizer.peekAhead(5)).toBe('ab');
      expect(tokenizer.position).toBe(0);
    });

    test('works with zero count', () => {
      const tokenizer = new Tokenizer('abc');
      expect(tokenizer.peekAhead(0)).toBe('');
    });
  });

  describe('skipWhitespace()', () => {
    test('skips spaces and tabs', () => {
      const tokenizer = new Tokenizer('   \t\t  abc');
      const count = tokenizer.skipWhitespace();
      expect(count).toBe(7);
      expect(tokenizer.peekChar()).toBe('a');
    });

    test('does not skip newlines', () => {
      const tokenizer = new Tokenizer('  \n  abc');
      const count = tokenizer.skipWhitespace();
      expect(count).toBe(2);
      expect(tokenizer.peekChar()).toBe('\n');
    });

    test('returns 0 if no whitespace', () => {
      const tokenizer = new Tokenizer('abc');
      const count = tokenizer.skipWhitespace();
      expect(count).toBe(0);
    });
  });

  describe('skipAllWhitespace()', () => {
    test('skips all whitespace including newlines', () => {
      const tokenizer = new Tokenizer('  \n\t\r  abc');
      const count = tokenizer.skipAllWhitespace();
      expect(count).toBe(7);
      expect(tokenizer.peekChar()).toBe('a');
    });

    test('returns 0 if no whitespace', () => {
      const tokenizer = new Tokenizer('abc');
      const count = tokenizer.skipAllWhitespace();
      expect(count).toBe(0);
    });
  });

  describe('isEOF()', () => {
    test('returns false when not at end', () => {
      const tokenizer = new Tokenizer('abc');
      expect(tokenizer.isEOF()).toBe(false);
      tokenizer.nextChar();
      expect(tokenizer.isEOF()).toBe(false);
    });

    test('returns true at end of input', () => {
      const tokenizer = new Tokenizer('a');
      tokenizer.nextChar();
      expect(tokenizer.isEOF()).toBe(true);
    });

    test('returns true for empty input', () => {
      const tokenizer = new Tokenizer('');
      expect(tokenizer.isEOF()).toBe(true);
    });
  });

  describe('isLineStart()', () => {
    test('returns true at start', () => {
      const tokenizer = new Tokenizer('abc');
      expect(tokenizer.isLineStart()).toBe(true);
    });

    test('returns false after reading characters', () => {
      const tokenizer = new Tokenizer('abc');
      tokenizer.nextChar();
      expect(tokenizer.isLineStart()).toBe(false);
    });

    test('returns true after newline', () => {
      const tokenizer = new Tokenizer('abc\ndef');
      while (tokenizer.peekChar() !== '\n') {
        tokenizer.nextChar();
      }
      tokenizer.nextChar(); // consume newline
      expect(tokenizer.isLineStart()).toBe(true);
    });
  });

  describe('getPosition()', () => {
    test('returns current position info', () => {
      const tokenizer = new Tokenizer('abc\ndef');
      const pos1 = tokenizer.getPosition();
      expect(pos1).toEqual({ position: 0, line: 1, column: 1 });

      tokenizer.nextChar(); // a
      tokenizer.nextChar(); // b
      const pos2 = tokenizer.getPosition();
      expect(pos2).toEqual({ position: 2, line: 1, column: 3 });
    });
  });

  describe('createToken()', () => {
    test('creates token with position info', () => {
      const tokenizer = new Tokenizer('test');
      const token = tokenizer.createToken(TokenType.TEXT, 'test');

      expect(token.type).toBe(TokenType.TEXT);
      expect(token.value).toBe('test');
      expect(token.line).toBe(1);
      expect(token.column).toBe(1);
      expect(token.position).toBe(0);
    });

    test('includes metadata if provided', () => {
      const tokenizer = new Tokenizer('test');
      const token = tokenizer.createToken(TokenType.HEADING, '#', { level: 1 });

      expect(token.metadata).toEqual({ level: 1 });
    });
  });

  describe('readUntil()', () => {
    test('reads until delimiter', () => {
      const tokenizer = new Tokenizer('hello world');
      const text = tokenizer.readUntil(' ');
      expect(text).toBe('hello');
      expect(tokenizer.peekChar()).toBe(' ');
    });

    test('consumes delimiter when requested', () => {
      const tokenizer = new Tokenizer('hello world');
      const text = tokenizer.readUntil(' ', true);
      expect(text).toBe('hello');
      expect(tokenizer.peekChar()).toBe('w');
    });

    test('handles multiple delimiters', () => {
      const tokenizer = new Tokenizer('hello, world');
      const text = tokenizer.readUntil([',', '.']);
      expect(text).toBe('hello');
      expect(tokenizer.peekChar()).toBe(',');
    });

    test('reads to EOF if delimiter not found', () => {
      const tokenizer = new Tokenizer('hello');
      const text = tokenizer.readUntil(' ');
      expect(text).toBe('hello');
      expect(tokenizer.isEOF()).toBe(true);
    });

    test('handles multi-character delimiters', () => {
      const tokenizer = new Tokenizer('hello]]world');
      const text = tokenizer.readUntil(']]');
      expect(text).toBe('hello');
      expect(tokenizer.peekAhead(2)).toBe(']]');
    });
  });

  describe('match()', () => {
    test('matches string at current position', () => {
      const tokenizer = new Tokenizer('[[link]]');
      expect(tokenizer.match('[[')).toBe(true);
    });

    test('returns false if no match', () => {
      const tokenizer = new Tokenizer('[[link]]');
      expect(tokenizer.match('{{')).toBe(false);
    });

    test('does not consume by default', () => {
      const tokenizer = new Tokenizer('[[link]]');
      tokenizer.match('[[');
      expect(tokenizer.position).toBe(0);
    });

    test('consumes when requested', () => {
      const tokenizer = new Tokenizer('[[link]]');
      tokenizer.match('[[', true);
      expect(tokenizer.position).toBe(2);
      expect(tokenizer.peekChar()).toBe('l');
    });
  });

  describe('expect()', () => {
    test('consumes string if matched', () => {
      const tokenizer = new Tokenizer('[[link]]');
      tokenizer.expect('[[');
      expect(tokenizer.position).toBe(2);
    });

    test('throws if not matched', () => {
      const tokenizer = new Tokenizer('[[link]]');
      expect(() => tokenizer.expect('{{')).toThrow(/Expected/);
    });

    test('error message includes position', () => {
      const tokenizer = new Tokenizer('[[link]]');
      try {
        tokenizer.expect('{{');
        fail('Should have thrown');
      } catch (err) {
        expect(err.message).toContain('line 1');
        expect(err.message).toContain('column 1');
      }
    });
  });

  describe('substring()', () => {
    test('returns substring of input', () => {
      const tokenizer = new Tokenizer('hello world');
      expect(tokenizer.substring(0, 5)).toBe('hello');
      expect(tokenizer.substring(6, 11)).toBe('world');
    });
  });

  describe('reset()', () => {
    test('resets tokenizer to beginning', () => {
      const tokenizer = new Tokenizer('abc');
      tokenizer.nextChar();
      tokenizer.nextChar();
      expect(tokenizer.position).toBe(2);

      tokenizer.reset();
      expect(tokenizer.position).toBe(0);
      expect(tokenizer.line).toBe(1);
      expect(tokenizer.column).toBe(1);
      expect(tokenizer.nextChar()).toBe('a');
    });
  });

  describe('Complex scenarios', () => {
    test('handles escaped brackets correctly', () => {
      const tokenizer = new Tokenizer('[[link]]');

      // Check for escaped brackets
      if (tokenizer.match('[[')) {
        tokenizer.match('[[', true); // consume
        const content = tokenizer.readUntil(']]');
        expect(content).toBe('link');
        tokenizer.match(']]', true); // consume
        expect(tokenizer.isEOF()).toBe(true);
      }
    });

    test('handles lookahead for complex tokens', () => {
      const tokenizer = new Tokenizer('[{INSERT plugin}]');

      // Peek ahead to determine token type
      if (tokenizer.peekAhead(2) === '[{') {
        tokenizer.match('[{', true);
        const pluginContent = tokenizer.readUntil('}]');
        expect(pluginContent).toBe('INSERT plugin');
      }
    });

    test('handles position tracking across complex operations', () => {
      const tokenizer = new Tokenizer('line1\nline2\nline3');

      // Read first line
      const line1 = tokenizer.readUntil('\n', true);
      expect(line1).toBe('line1');
      expect(tokenizer.line).toBe(2);
      expect(tokenizer.column).toBe(1);

      // Read second line
      const line2 = tokenizer.readUntil('\n', true);
      expect(line2).toBe('line2');
      expect(tokenizer.line).toBe(3);
      expect(tokenizer.column).toBe(1);
    });
  });

  describe('TokenType enum', () => {
    test('exports all token types', () => {
      expect(TokenType.TEXT).toBe('TEXT');
      expect(TokenType.ESCAPED).toBe('ESCAPED');
      expect(TokenType.VARIABLE).toBe('VARIABLE');
      expect(TokenType.PLUGIN).toBe('PLUGIN');
      expect(TokenType.LINK).toBe('LINK');
      expect(TokenType.HEADING).toBe('HEADING');
      expect(TokenType.EOF).toBe('EOF');
    });
  });
});
